% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizers.R
\name{ADAM_optim}
\alias{ADAM_optim}
\title{Adaptive Moment Estimation (Adam) optimizer}
\usage{
ADAM_optim(learning_rate = 0.1, beta1 = 0.9, beta2 = 0.999, eps = 1e-08)
}
\arguments{
\item{learning_rate}{A numeric value specifying the initial learning rate. Defaults to 0.001.}

\item{beta1}{A numeric value between 0 and 1 controlling the exponential decay rate for the first moment estimate. Defaults to 0.9}

\item{beta2}{A numeric value between 0 and 1 controlling the exponential decay rate for the second moment estimate. Defaults to 0.999}

\item{eps}{A small numeric value for numerical stability. Defaults to 1e-8.}
}
\value{
A function performing the Adam update step.
}
\description{
This function implements the Adam optimizer, an adaptive learning rate optimization algorithm
that estimates first and second moments of the gradient to adjust learning rates.
}
\examples{
# Example usage
optimizer <- ADAM_optim(learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999)
# ... use the optimizer within your training loop
}
