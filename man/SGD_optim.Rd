% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizers.R
\name{SGD_optim}
\alias{SGD_optim}
\title{Stochastic Gradient Descent with momentum}
\usage{
SGD_optim(
  learning_rate = 0.1,
  momentum = 0,
  nesterov = FALSE,
  decay = "constant"
)
}
\arguments{
\item{learning_rate}{A numeric value specifying the initial learning rate. Defaults to 0.1.}

\item{momentum}{A numeric value between 0 and 1 controlling the momentum term. Defaults to 0.}

\item{nesterov}{A logical value indicating whether to use Nesterov accelerated gradients. Defaults to \code{FALSE}.}

\item{decay}{A character string specifying the learning rate decay schedule:
\itemize{
\item "constant" (default): No decay.
\item "linear": Linear decay over iterations.
\item "inv_sqrt": Inverse square root decay.
\item "cosine": Cosine annealing decay.
}}
}
\value{
A function performing the SGD update step.
}
\description{
This function implements the Stochastic Gradient Descent (SGD) optimizer with momentum
and optional Nesterov accelerated gradients and learning rate decay.
}
\examples{
# Example 1: SGD with constant learning rate and momentum
optimizer <- SGD_optim(learning_rate = 0.01, momentum = 0.9)

# Example 2: SGD with linear learning rate decay
optimizer <- SGD_optim(learning_rate = 0.1, decay = "linear")

# ... similar examples for other decay options
}
